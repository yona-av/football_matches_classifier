{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4f9c8b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import sys\n",
    "import seaborn as sns\n",
    "import scipy as sp\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel \n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1939e479",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_original = pd.read_csv(r\"C:/Users/Yehonatan/PycharmProject/DS/la_liga_data.csv\")\n",
    "#df_original.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "1b31f6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def deal_nans(df, percantage_nan):\n",
    "    nans = df[df.columns[df.isnull().any()]]\n",
    "    nan_col_names = list(nans)\n",
    "    for i in nan_col_names:\n",
    "        count_nan = df[i].isnull().sum()\n",
    "        if count_nan/len(df[i]) >= 0.1:       # if nans are more than 10% delete the column \n",
    "            df = df.drop(columns=[i])\n",
    "        else:\n",
    "            col_avg = df[i].mean()\n",
    "            df[i] = df[i].fillna(col_avg)\n",
    "    return df \n",
    "\n",
    "\n",
    "# create a function that gets the features and target . calcs the p-values and if it rejects or accepts H_0 null hypothesis \n",
    "\n",
    "def pvalue_filter(target, features, alpha): # returns a list of columns that are possible drop, p_val > alpha, corr, pval\n",
    "    features_columns_names = list(features)\n",
    "    target_column_name = list(target)\n",
    "    features_np = features.to_numpy()\n",
    "    target_np = target.to_numpy()\n",
    "    drop_index = []\n",
    "    p_val_list =[]\n",
    "    corr_list = []\n",
    "    \n",
    "    for i in range(len(features_columns_names)):\n",
    "        corr, p_val = sp.stats.pearsonr(features_np[:,i], target_np)\n",
    "        corr_list.append(round(corr,3))\n",
    "        if p_val > alpha:           # accept the null hypothesis, no statisitcal significance  \n",
    "            drop_index.append(i)\n",
    "            p_val_list.append(p_val)\n",
    "            \n",
    "    drop_col = [features_columns_names[i] for i in drop_index]\n",
    "    return drop_col, corr_list, p_val_list\n",
    "\n",
    "# create a function that returns the selected and rejected columns by correlation between features \n",
    "def features_corr_filter(features, corr_cutoff): \n",
    "    corr = features.corr()\n",
    "    columns = np.full((corr.shape[0],), True, dtype=bool) # create boolean filter \n",
    "\n",
    "    for i in range(corr.shape[0]):\n",
    "        for j in range(i+1, corr.shape[0]):\n",
    "            if corr.iloc[i,j] >= corr_cutoff:\n",
    "                columns[j] = False\n",
    "\n",
    "    rejected_columns = x_train.columns[np.invert(columns)]\n",
    "    #print('reject',rejected_columns)\n",
    "    selected_columns = x_train.columns[columns]\n",
    "    #print('selected',selected_columns)\n",
    "    return selected_columns, rejected_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "e80f7d09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###### EDA and data processing ###############\n",
    "\n",
    "# turn data into numerical float64 \n",
    "#create Feature selectin \n",
    "\n",
    "df = df_original.copy()\n",
    "# deal with object type attributes \n",
    "col_list = df.select_dtypes(include=['object']).columns.to_list()\n",
    "df = df.drop(columns = ['start_time', 'round', 'dayofweek', 'opponent'])\n",
    "df['venue']= df['venue'].apply(lambda x : 1 if x== 'Home' else 0) \n",
    "\n",
    "# for now we turn this classifier to be a WIN classifier so OvA strategy\n",
    "df['result']= df['result'].apply(lambda x : 1 if x== 'W' else 0) \n",
    "\n",
    "#turn df into float 64 \n",
    "df=df.astype('float64')\n",
    "\n",
    "# deal with NAN values \n",
    "# special columns to deal with\n",
    "df['gk_save_pct'] = df['gk_save_pct'].fillna(100) # no shots on target means no saves in a way same effect as 100% saves\n",
    "df['own_goals'] = df['own_goals'].fillna(0) # safe to assume that if there is a NAN there were no owngoals as it is a rare occasion \n",
    "df = df.drop(columns=['tackles_interceptions', 'Unnamed: 0']) # all Nan_s in this column\n",
    "\n",
    "deal_nans(df, 0.1)\n",
    "\n",
    "\n",
    "#count_nan = df['tackles_interceptions'].isnull().sum()\n",
    "#print('Number of NaN values present: ' + str(count_nan))\n",
    "\n",
    "# divide to x,y sets \n",
    "y = df['result']\n",
    "x = df.drop(['result'], axis=1)\n",
    "\n",
    "# divide into test set and train set   \n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=9)\n",
    "\n",
    "train_set = pd.concat([x_train,y_train],axis=1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "229627c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### APPLY FEATURE SELECTION ##########\n",
    "\n",
    "# drop the statisticly insignificant features \n",
    "drop_col, corr_list, p_val_list = pvalue_filter(y_train, x_train, 0.05)\n",
    "#print(drop_col)\n",
    "x_train = x_train.drop(drop_col, axis=1)\n",
    "x_train.rename(columns = {'passes.1':'attempted_passes'}, inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "abb8a877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop features that are by product of different circumstances like xg related features\n",
    "# leave features that are controllable - meaning can be worked and controlled by the team\n",
    "\n",
    "hand_pick_drop = ['goals_for','goals_against','gk_psxg_net', 'assists', 'passes_left_foot', 'passes_right_foot',\n",
    "                  'passes_head','goals','goals_per_shot', 'goals_per_shot_on_target',\n",
    "                  'xg', 'npxg', 'npxg_per_shot', 'xg_net', 'npxg_net']\n",
    "\n",
    "x_train = x_train.drop(hand_pick_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "21980066",
   "metadata": {},
   "outputs": [],
   "source": [
    "### apply corr filter on x_train #### might want to check performance without it\n",
    "#might reduce errors and process time and run time \n",
    "\n",
    "selected_columns, rejected_columns = features_corr_filter(x_train, 0.9)\n",
    "x_train = x_train[selected_columns]\n",
    "#x_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "58c99325",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### after all feature selection filtering\n",
    "### now x_train has 84 features (half of the beginning) #### pvalue drop has been made and high corr between features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "ae7e8440",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### now we scale the data with standardscaler ##### prepre it for learning, convert to np array\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train_np = x_train.to_numpy()   \n",
    "x_train_np = scaler.fit_transform(x_train_np)\n",
    "\n",
    "x_train_df = pd.DataFrame(x_train_np, columns= list(selected_columns)) # df after scaling\n",
    "y_train_np = y_train.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "9cf54760",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### apply classifier ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "e704497e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators= 100, max_depth=2, random_state=0)\n",
    "clf.fit(x_train_np, y_train_np)\n",
    "y_predict = clf.predict(x_train_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "cc3ab57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.824\n",
      "precision : 0.937\n",
      "recall : 0.624\n",
      "f_1 : 0.749\n"
     ]
    }
   ],
   "source": [
    "#### check the accuracy, precision and recall ###### all feature slection is applied\n",
    "## so far it is the best result because I want high precision , \n",
    "# if the model classified it as a win I want it to missclassify it as much as it can. i dont care if it missclassfies it as a lose \n",
    "\n",
    "precision, recall, f_1, support = precision_recall_fscore_support(y_train_np, y_predict, average='binary')\n",
    "accuracy = accuracy_score(y_train_np, y_predict)\n",
    "print('accuracy :', np.round(accuracy,3))\n",
    "print('precision :', np.round(precision,3))\n",
    "print('recall :', np.round(recall,3))\n",
    "print('f_1 :', np.round(f_1,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "d40301c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      0.97      0.87      1147\n",
      "         1.0       0.94      0.62      0.75       829\n",
      "\n",
      "    accuracy                           0.82      1976\n",
      "   macro avg       0.86      0.80      0.81      1976\n",
      "weighted avg       0.85      0.82      0.82      1976\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train_np, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "43620f0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'ccp_alpha': 0.0,\n",
       " 'class_weight': None,\n",
       " 'criterion': 'gini',\n",
       " 'max_depth': 2,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_leaf_nodes': None,\n",
       " 'max_samples': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': None,\n",
       " 'oob_score': False,\n",
       " 'random_state': 0,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e57eaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0856e07c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "4af38e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train_df_scale.hist(bins=50, figsize=(20,15))\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "8bed9eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train['passes_switches'].hist(bins=50, figsize=(20,15))\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "851acac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train.nutmegs.value_counts() \n",
    "#x_train['gk_clean_sheets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afb52d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07c45cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### check model predictions without features elimination "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963f950d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983df892",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f9e18b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f32c7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
